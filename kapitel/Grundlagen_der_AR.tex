\section{Technische Grundlagen der Augmented Reality}

Die Grundlage eines funktionsfähigen Augmented Reality-Systems bildet ein
vielfältiges Zusammenspiel verschiedener technischer Systeme und Verfahren.
Diese können grob in folgende Bereiche unterteilt werden, die zusammenarbeiten,
um eine nahtlose und immersive AR-Erfahrung zu ermöglichen.

\subsection{Sensoren und Erfassungstechnologien}

Um virtuelle Inhalte nahtlos in die reale Welt zu integrieren, ist eine präzise
Bestimmung der Position des AR-Systems im Raum erforderlich. Dazu werden
verschiedene Sensoren und Erfassungstechnologien eingesetzt, die die Umgebung
analysieren und die Position sowie Ausrichtung des AR-Geräts ermitteln. Häufig
werden optische Tracking-Verfahren wie Infrarot-Sensoren, 3D-Kameras und
herkömmliche Kameras im sichtbaren Lichtspektrum aufgrund ihrer geringen Kosten
und hohen Verfügbarkeit verwendet. Diese Sensoren erfassen visuelle
Informationen aus der Umgebung und dienen zur präzisen Bestimmung der Position
und Ausrichtung des AR-Geräts anhand von Markern oder speziellen Merkmalen.
Besonders beliebt sind in den letzten Jahren 3D-Kamerasysteme, die auf
structured Light oder Time of Flight basieren. Durch fortschrittliche
Bildverarbeitungsalgorithmen können visuelle Elemente genau erkannt und
verfolgt werden, um virtuelle Objekte in Echtzeit in die reale Welt zu
integrieren. Für den Außenbereich wird häufig auch GPS (Global Positioning
System) verwendet, wobei die Genauigkeit je nach Anwendungsbereich moderat sein
kann und Herausforderungen mit sich bringen kann.

Ein weiterer wichtiger Bestandteil im Bereich der AR sind Inertiale
Messeinheiten (IMUs). Diese Sensoren messen Beschleunigung,
Winkelgeschwindigkeit und magnetische Felder. Durch die Integration von IMUs in
AR-Geräte können Bewegungen und Rotationen des Geräts erfasst und verfolgt
werden, um die relative Position im Raum zu bestimmen. IMUs sind unempfindlich
gegenüber äußeren Störeinflüssen und erfordern keine direkte Sichtlinie zu
anderen Sensoren. Allerdings besteht das Problem, dass sich die gemessene
Position und Ausrichtung im Laufe der Zeit verschieben und von der
tatsächlichen Position abweichen können. Daher werden IMUs häufig in
Kombination mit anderen Sensoren wie 3D-Kameras verwendet, um präzisere
Ergebnisse zu erzielen. Eine erfolgreiche Tracking-Methode basiert grundlegend
auf der Kombination verschiedener Tracking-Verfahren und deren Vorteilen, was
als Sensorfusion bekannt ist. \cite{billinghurst2015survey}
\subsection{Sensorfusion}
Sensorfusion im Bereich der erweiterten Realität (AR) bezieht sich auf die
kombinierte Verwendung mehrerer Sensoren, um genaue und zuverlässige
Informationen über die Position und Ausrichtung eines AR-Geräts in der realen
Welt zu erhalten. Ein häufig verwendeter Ansatz für die Sensorfusion ist das
erweiterte Kalman-Filter (EKF), das auf dem Kalman-Filter basiert und speziell
für nichtlineare Systeme entwickelt wurde.

Der EKF verwendet mathematische Modelle, um die Bewegung und das Verhalten der
Sensoren zu beschreiben. Durch die Kombination der Messungen aus verschiedenen
Sensoren werden die Vorteile jedes Sensors genutzt, um eine robuste und genaue
Schätzung der Position und Ausrichtung zu erhalten. Dabei werden die
Unsicherheiten und Fehler der einzelnen Sensoren berücksichtigt und korrigiert.

Die Funktionsweise des EKF beruht auf einer Vorhersage- und einer
Aktualisierungsphase. In der Vorhersagephase werden die aktuellen Schätzungen
der Position und Ausrichtung basierend auf den vorherigen Messungen und dem
mathematischen Modell der Bewegung des AR-Geräts prognostiziert. In der
Aktualisierungsphase werden die aktuellen Messungen der Sensoren verwendet, um
die Vorhersagen zu korrigieren und eine genauere Schätzung zu erhalten.

Ein wichtiger Aspekt des EKF ist die Berücksichtigung der Unsicherheit der
Sensormessungen und des mathematischen Modells. Durch die Anpassung der
Gewichtung der einzelnen Messungen und die Schätzung der Unsicherheit wird eine
robuste Fusion der Sensordaten erreicht.

Die Sensorfusion mithilfe des EKF ermöglicht es, genaue und zuverlässige
Informationen über die Position und Ausrichtung eines AR-Geräts in Echtzeit zu
erhalten. Dies ist entscheidend für eine nahtlose Integration von virtuellen
Inhalten in die reale Welt und ermöglicht eine immersive AR-Erfahrung für die
Benutzer. \cite{5336489,simon2006optimal}


Die mathematischen Gleichungen des EKF sind wie folgt definiert:
\begin{enumerate}
      \item Vorhersage des Zustands:
            \begin{equation*}
                  \hat{x}_{k+1} = g({x}_{k}, u)\
            \end{equation*}
            wobei $g$ die Zustandsübergangsfunktion,
            $x_{k}$ der geschätzte Zustand zum Zeitpunkt $k$ 
            und $u$ die Eingabe zum Zeitpunkt $k$ ist.\\
      \item Vorhersage der Fehlerkovarianz:
            \begin{equation*}
                  P_{k + 1} = J_{A} P_{k} A_{A}^T + Q\
            \end{equation*}
            wobei $J_{A}$ die Jacobi-Matrix der Zustandsübergangsfunktion,
            $P_{k}$ die Kovarianzmatrix des Schätzfehlers 
            und $Q$ die Kovarianzmatrix des Prozessrauschens ist.\\
      \item Berechnung der Kalman Verstärkung:
            \begin{equation*}
                  K_{k} = P_{k} J_{H}^T (J_{H} P_{k} J_{H}^T + R)^{-1}\
            \end{equation*}
            wobei $K_{k} $ die Kalman-Verstärkung,
            $J_{H}$ die Jacobi-Matrix der Messfunktion,
            $R$ die Kovarianzmatrix des Messrauschens und
            $z_{k}$ die Messung zum Zeitpunkt $k$ ist.\\
      \item Aktualisierung der Schätzung durch Messung:
            \begin{equation*}
                  \hat{x}_{k} = \hat{x}_{k} + K_{z} (z_{k} - h (\hat{x}_{k})\
            \end{equation*}
            wobei $J_{H}$ die Jacobi-Matrix der Messfunktion,
            $R$ die Kovarianzmatrix des Messrauschens
            und $z_{k}$ die Messung zum Zeitpunkt $k$ ist.\\

      \item Korrektur der Fehlerkovarianz:
            \begin{equation*}
                  P_{k} = (I - K_{k} J_{H}) P_{k}\
            \end{equation*}
            wobei $I$ die Einheitsmatrix ist.\\
\end{enumerate}

\subsection{Datenverarbeitung und -darstellung}
Die Datenverarbeitung und Darstellung spielen eine entscheidende Rolle im
Bereich der Augmented Reality (AR). Um eine nahtlose Integration von virtuellen
Inhalten in die reale Welt zu ermöglichen, müssen die erfassten Daten zunächst
verarbeitet und interpretiert werden. Anschließend erfolgt die Darstellung der
AR-Inhalte in einer für den Benutzer verständlichen Form. Dabei ist eine
Kalibrierung der Tracking-Systeme, wie beispielsweise der Kamera, erforderlich,
um genaue Positionierungsinformationen zu erhalten.

Die Darstellung der AR-Inhalte erfolgt in Echtzeit, um eine immersive und
interaktive Erfahrung zu gewährleisten. Hierbei spielen Grafiktechnologien wie
Computergrafik, Rendering-Algorithmen und Schattenberechnung eine wichtige Rolle. Die
virtuellen Objekte müssen realistisch und überzeugend in die reale Umgebung
integriert werden. Dies erfordert die Berücksichtigung von Aspekten wie
Beleuchtung, Schatten und Perspektive, um eine konsistente und immersive
AR-Erfahrung zu schaffen.

Es gibt verschiedene Arten, wie die Darstellung der AR-Inhalte erfolgen kann.
Bei der Video-gestützten Variante wird die Kamera des AR-Geräts verwendet, um
eine Echtzeit-Videoaufnahme der Umgebung zu erfassen und auf dem Display
anzuzeigen. Die AR-Inhalte werden mithilfe von Bildverarbeitungstechniken in
die realen Aufnahmen integriert. Der schematische Aufbau eines solchen
Verfahrens ist in Abbildung \ref{fig:VBAR} dargestellt.\\

\begin{figure}[h]
      \centering
      \includesvg[width=0.7\columnwidth]{bilder/svg/strucutre_video_based_AR.svg}
      \caption[width=0.7\columnwidth]{Aufbau von Video basierten AR-Displays \cite{billinghurst2015survey}}
      \label{fig:VBAR}
\end{figure}

Ein weiteres Verfahren, das in Abbildung 2 dargestellt ist, basiert auf
optischen, durchsichtigen AR-Displays, die auf halbdurchlässigen Spiegeln
basieren. Bei dieser Variante kann der Benutzer die reale Welt durch den
halbdurchlässigen Spiegel sehen und gleichzeitig die reflektierte Anzeige der
AR-Inhalte wahrnehmen. Die Besonderheit dieser Technologie liegt in der
Anordnung des halbdurchlässigen Spiegels unter einem Winkel von 45 Grad.
Dadurch wird das einfallende Licht sowohl reflektiert als auch teilweise
durchgelassen. Ein bekanntes Anwendungsbeispiel für diese Technologie sind
Head-up-Displays in Autos, bei denen relevante Informationen wie
Geschwindigkeit oder Navigationshinweise direkt auf die Windschutzscheibe
projiziert werden. Diese Art der Darstellung ermöglicht es dem Benutzer, die
AR-Inhalte zu sehen, ohne den Blick von der Straße abwenden zu müssen. Auch
erste Versuche von AR-Brillen, wie den Google Glass basierten auf dieser
Technologie.\\

\begin{figure}[h]
      \centering
      \includesvg[width=0.7\columnwidth]{bilder/svg/optical_see_trough_display.svg}
      \caption[width=0.7\columnwidth]{Aufbau von optischen, durchsichtigen AR-Displays \cite{billinghurst2015survey}}
      \label{fig:OSTAR}
\end{figure}

Eine weitere Variante nutzt einen Projektor, um Texturen oder Bilder auf
bestehende Objekte zu projizieren und somit die realen Objekte zu erweitern.
Durch diese Methode werden zusätzliche visuelle Informationen auf die
Oberfläche der Objekte übertragen, um beispielsweise Anleitungen oder virtuelle
Beschriftungen darzustellen.

Diese verschiedenen Technologien der AR-Darstellung können auf unterschiedliche
Arten für den Benutzer zugänglich gemacht werden. Eine häufig verwendete Form
sind sogenannte Head-Mounted-Displays, wie AR-Brillen. Diese ermöglichen es dem
Benutzer, die AR-Inhalte direkt vor seinen Augen wahrzunehmen und sie nahtlos
in die reale Welt einzubetten. Durch das Tragen der AR-Brille hat der Benutzer
die Freiheit, sich in der Umgebung zu bewegen und die AR-Erfahrung ohne
Einschränkungen zu erleben. \cite{billinghurst2015survey}

\subsection{Benutzerschnittstellen und Interaktion}
Die Benutzerschnittstellen und Interaktion spielen eine zentrale Rolle im
Bereich der Augmented Reality (AR) und tragen maßgeblich zur intuitiven
Bedienbarkeit, Benutzerfreundlichkeit und Immersion von AR-Anwendungen bei.

Head-Mounted Display (HMD) ermöglichen es dem Benutzer, die virtuellen Inhalte
direkt vor seinen Augen zu sehen. Das HMD kann mit Sensoren ausgestattet sein,
um die Umgebung, sowie Bewegungen des Nutzers zu erkennen. Dies ermöglicht eine
Reihe von Interaktionsmöglichkeiten mit der AR-Anwendung.
\begin{itemize}
      \item 2D User Interfaces: Bei dieser Form der Interaktion,
            werden physische Knöpfe und Tasten verwendet, um mit den virtuellen Inhalten zu interagieren.
            Dies umfasst beispielsweise das Auswählen von Objekten, das Ausführen von Aktionen oder das Navigieren in Menüs. Auch Touch-Eingaben sind möglich.

      \item Für eine erweiterte Interaktionsmöglichkeit werden 3D-Benutzerschnittstellen
            verwendet, die Manipulationen an Objekten mit sechs Freiheitsgraden
            ermöglichen. Dies umfasst das Bewegen, Drehen und Skalieren von Objekten. Die
            Eingabegeräte selbst können unterschiedliche Formen annehmen, wie
            beispielsweise 3D-Mäuse oder Stäbe.

      \item Gestensteuerung: Bei dieser fortschrittlichen Form der Interaktion werden
            Gesten, Handbewegungen und auch Kopfbewegungen verwendet, um mit den virtuellen
            Inhalten zu interagieren. Dies umfasst das Zeigen auf Objekte, das Ziehen und
            Drehen von Objekten sowie das Ausführen von Gesten wie Pinch-to-Zoom. Darüber
            hinaus können Kopfbewegungen zur Steuerung von Menüs, zum Navigieren durch
            virtuelle Umgebungen oder zum Anpassen von Blickwinkeln genutzt werden. Durch
            die Einbindung von Kopfbewegungen in die Gestensteuerung wird eine natürlichere
            und immersivere Interaktion mit den AR-Inhalten ermöglicht. So kann der
            Benutzer beispielsweise durch Drehen des Kopfes seine Perspektive in der
            erweiterten Realität ändern oder durch Kopfbewegungen Menüoptionen auswählen.
            Diese erweiterte Form der Gestensteuerung trägt dazu bei, die Interaktion mit
            AR-Inhalten intuitiver und realitätsnäher zu gestalten. Auch Eye-Tracking ist
            hierbei möglich, um Objekte durch das Ansehen auszuwählen.

      \item Sprachbefehle: Auch Sprachbefehle können genutz werden, um mit Objekten im Raum
            zu interagieren. Benutzer können bestimmte Sprachbefehle verwenden, um Aktionen
            auszuführen, Objekte zu steuern oder Informationen abzurufen. Diese Art der
            Interaktion kann besonders nützlich sein, wenn die Hände des Benutzers
            beschäftigt sind.
\end{itemize}

Die Benutzerschnittstellen und Interaktionsmethoden in der AR werden
kontinuierlich weiterentwickelt, um die Benutzererfahrung zu verbessern und
neue Möglichkeiten der Interaktion zu erschließen. Durch die Integration von
fortgeschrittenen Technologien wie maschinellem Lernen und Künstlicher
Intelligenz ist es möglich, natürlichere und kontextbezogene Interaktionen zu
ermöglichen. Die Gestaltung der Benutzerschnittstellen und Interaktionsmethoden
spielt eine entscheidende Rolle, um AR-Anwendungen zugänglich,
benutzerfreundlich und ansprechend zu gestalten und den Benutzern ein
immersives und interaktives Erlebnis zu bieten. \cite{billinghurst2015survey}s