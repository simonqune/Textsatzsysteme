\section{Technische Grundlagen der Augmented Reality}

Die Grundlage eines funktionsfähigen Augmented Reality-Systems bildet ein
vielfältiges Zusammenspiel verschiedener technischer Systeme und Verfahren.
Diese können grob in folgende drei wesentliche Bereiche unterteilt werden, die
zusammenarbeiten, um eine nahtlose und immersive AR-Erfahrung zu ermöglichen.

\subsection{Sensoren und Erfassungstechnologien}

Um virtuelle Inhalte nahtlos in die reale Welt zu integrieren, ist eine präzise
Bestimmung der Position des AR-Systems im Raum erforderlich. Dazu werden
verschiedene Sensoren und Erfassungstechnologien eingesetzt, die die Umgebung
analysieren und die Position sowie Ausrichtung des AR-Geräts ermitteln.
Optische Tracking-Verfahren wie Infrarot-Sensoren, 3D-Kameras und herkömmliche
Kameras im sichtbaren Lichtspektrum werden aufgrund ihrer geringen Kosten und
hohen Verfügbarkeit häufig verwendet. Sie erfassen visuelle Informationen aus
der Umgebung und dienen zur Erkennung von Markern oder speziellen Merkmalen, um
die Position und Ausrichtung des AR-Geräts präzise zu bestimmen. Insbesondere
3D-Kamerasysteme, die auf structured Light oder Time of Flight basieren, sind
in den letzten Jahren immer beliebter geworden. Mithilfe fortschrittlicher
Bildverarbeitungsalgorithmen erfolgt eine genaue Erkennung und Verfolgung
visueller Elemente, um virtuelle Objekte in Echtzeit in die reale Welt zu
integrieren.

Ein weiterer wichtiger Bestandteil im Bereich der AR sind Inertiale
Messeinheiten (IMUs). Diese Sensoren messen Beschleunigung,
Winkelgeschwindigkeit und magnetische Felder. Durch die Integration von IMUs in
AR-Geräte können Bewegungen und Rotationen des Geräts erfasst und verfolgt
werden, um die relative Position im Raum zu bestimmen. IMUs sind unempfindlich
gegenüber äußeren Störeinflüssen und erfordern keine direkte Sichtlinie zu
anderen Sensoren. Es besteht jedoch das Problem, dass sich die gemessene
Position und Ausrichtung im Laufe der Zeit verschiebt und von der tatsächlichen
Position abweicht. Daher werden IMUs häufig in Kombination mit anderen Sensoren
wie 3D-Kameras verwendet, um präzisere Ergebnisse zu erzielen. Grundlegend für
ein erfolgreiches Tracking ist die Kombination aus verschiedenene Tracking
Verfahren und deren Vorteilen. Diese Kombination ist unter Sensor Fusion
bekannt.

UM GPS ERWETERN

\subsection{Sensorfusion}
Die Sensorfusion spielt eine entscheidende Rolle im Bereich der erweiterten
Realität (AR), da sie es ermöglicht, genaue und konsistente virtuelle
Darstellungen der realen Welt zu erzeugen. In dem vorliegenden Paper von XYZ et
al.wird eine Methode zur Sensorfusion basierend auf dem erweiterten
Kalman-Filter (EKF) vorgestellt. Der EKF ist ein mathematisches Modell, das den
Zustand eines Systems schätzt und dabei sowohl Messungen als auch vorherige
Zustandsschätzungen berücksichtigt.

Die vorgeschlagene Methode kombiniert die Daten verschiedener Sensoren wie
Kamera, Inertialsensoren und Tiefensensor, um die räumliche Position und
Orientierung des AR-Geräts zu bestimmen. Hierbei werden mathematische
Gleichungen verwendet, um die Sensorausgaben in ein gemeinsames
Koordinatensystem zu transformieren und anschließend zu fusionieren. Der EKF
verwendet dabei die Bewegungs- und Messmodelle der Sensoren, um eine optimale
Schätzung des Zustands des AR-Geräts zu generieren.

Die mathematischen Gleichungen des EKF sind wie folgt definiert:
\begin{enumerate}
      \item Vorhersage des Zustands:
            \begin{equation*}
                  \hat{x}_{k+1} = g({x}_{k}, u)\
            \end{equation*}
            wobei $g$ die Zustandsübergangsfunktion ist,
            $x_{k}$ der geschätzte Zustand zum Zeitpunkt $k$ ist
            und $u$ die Eingabe zum Zeitpunkt $k$ ist.\\
      \item Vorhersage der Fehlerkovarianz:
            \begin{equation*}
                  P_{k + 1} = J_{A} P_{k} A_{A}^T + Q\
            \end{equation*}
            wobei $J_{A}$ die Jacobi-Matrix der Zustandsübergangsfunktion ist, 
            $P_{k}$ die Kovarianzmatrix des Schätzfehlers ist 
            und $Q$ die Kovarianzmatrix des Prozessrauschens ist.\\
      \item Berechnung der Kalman Verstärkung:
            \begin{equation*}
                  K_{k} = P_{k} J_{H}^T (J_{H} P_{k} J_{H}^T + R)^{-1}\
            \end{equation*}
            wobei $K_{k} $ die Kalman-Verstärkung ist,
             $J_{H}$ die Jacobi-Matrix der Messfunktion ist,
              $R$ die Kovarianzmatrix des Messrauschens ist und 
              $z_{k}$ die Messung zum Zeitpunkt $k$ ist.\\
      \item Aktualisierung der Schätzung durch Messung:
            \begin{equation*}
                  \hat{x}_{k} = \hat{x}_{k} + K_{z} (z_{k} - h (\hat{x}_{k})\
            \end{equation*}
            wobei $J_{H}$ die Jacobi-Matrix der Messfunktion ist,
             $R$ die Kovarianzmatrix des Messrauschens ist
              und $z_{k}$ die Messung zum Zeitpunkt $k$ ist.\\

      \item Korrektur der Fehlerkovarianz:
            \begin{equation*}
                  P_{k} = (I - K_{k} J_{H}) P_{k}\
            \end{equation*}
            wobei $I$ die Einheitsmatrix ist.\\
\end{enumerate}

Die vorgestellte Methode zur Sensorfusion im AR-Bereich basierend auf dem EKF
ermöglicht eine präzise Bestimmung der räumlichen Position und Orientierung des
AR-Geräts. Durch die Integration verschiedener Sensoren und die Anwendung
mathematischer Gleichungen des EKF können Messungen korrigiert und Fehler
reduziert werden. Die resultierenden Schätzungen tragen dazu bei, eine nahtlose
Verschmelzung von virtuellen und realen Inhalten in AR-Anwendungen zu
ermöglichen.

\subsection{Datenverarbeitung und -darstellung}
Die Datenverarbeitung und Darstellung spielen eine entscheidende Rolle im
Bereich der Augmented Reality (AR). Um eine nahtlose Integration von virtuellen
Inhalten in die reale Welt zu ermöglichen, müssen die erfassten Daten zunächst
verarbeitet und interpretiert werden. Anschließend erfolgt die Darstellung der
AR-Inhalte in einer für den Benutzer verständlichen Form. Dabei ist eine
Kalibrierung der Tracking-Systeme, wie beispielsweise der Kamera, erforderlich,
um genaue Positionierungsinformationen zu erhalten.

Die Darstellung der AR-Inhalte erfolgt in Echtzeit, um eine immersive und
interaktive Erfahrung zu gewährleisten. Hierbei spielen Grafiktechnologien wie
Computergrafik, Rendering-Algorithmen und Shading eine wichtige Rolle. Die
virtuellen Objekte müssen realistisch und überzeugend in die reale Umgebung
integriert werden. Dies erfordert die Berücksichtigung von Aspekten wie
Beleuchtung, Schatten und Perspektive, um eine konsistente und immersive
AR-Erfahrung zu schaffen.

Es gibt verschiedene Arten, wie die Darstellung der AR-Inhalte erfolgen kann.
Bei der video-gestützten Variante wird die Kamera des AR-Geräts verwendet, um
eine Echtzeit-Videoaufnahme der Umgebung zu erfassen und auf dem Display
anzuzeigen. Die AR-Inhalte werden mithilfe von Bildverarbeitungstechniken in
die realen Aufnahmen integriert. Der schematische Aufbau eines solchen
Verfahrens ist in Abbildung \ref{fig:VBAR} dargestellt.

\begin{figure}[h]
      \centering
      \includesvg[width=0.7\columnwidth]{bilder/svg/strucutre_video_based_AR.svg}
      \caption[width=0.7\columnwidth]{Aufbau von Video basierten AR-Displays \cite{billinghurst2015survey}}
      \label{fig:VBAR}
\end{figure}

Ein weiteres Verfahren, das in Abbildung 2 dargestellt ist, basiert auf
optischen, durchsichtigen AR-Displays, die auf halbdurchlässigen Spiegeln
basieren. Bei dieser Variante kann der Benutzer die reale Welt durch den
halbdurchlässigen Spiegel sehen und gleichzeitig die reflektierte Anzeige der
AR-Inhalte wahrnehmen. Die Besonderheit dieser Technologie liegt in der
Anordnung des halbdurchlässigen Spiegels unter einem Winkel von 45 Grad.
Dadurch wird das einfallende Licht sowohl reflektiert als auch teilweise
durchgelassen. Ein bekanntes Anwendungsbeispiel für diese Technologie sind
Head-up-Displays in Autos, bei denen relevante Informationen wie
Geschwindigkeit oder Navigationshinweise direkt auf die Windschutzscheibe
projiziert werden. Diese Art der Darstellung ermöglicht es dem Benutzer, die
AR-Inhalte zu sehen, ohne den Blick von der Straße abwenden zu müssen. Auch
erste Versuche von AR-Brillen, wie den goggle glasses basierten auf dieser
Technologie.

\begin{figure}[h]
      \centering
      \includesvg[width=0.7\columnwidth]{bilder/svg/optical_see_trough_display.svg}
      \caption[width=0.7\columnwidth]{Aufbau von optischen, durchsichtigen AR-Displays \cite{billinghurst2015survey}}
      \label{fig:OSTAR}
\end{figure}

Eine weitere Variante nutzt einen Projektor, um Texturen oder Bilder auf
bestehende Objekte zu projizieren und somit die realen Objekte zu erweitern.
Durch diese Methode werden zusätzliche visuelle Informationen auf die
Oberfläche der Objekte übertragen, um beispielsweise Anleitungen oder virtuelle
Beschriftungen darzustellen. statische vs mobile SYSTEME

Diese verschiedenen Technologien der AR-Darstellung können auf verschiedene
Arten für den Benutzer zugänglich gemacht werden. Eine häufig verwendete Form
sind sogenannte Head-Mounted-Displays, wie AR-Brillen. Diese ermöglichen es dem
Benutzer, die AR-Inhalte direkt vor seinen Augen wahrzunehmen und sie nahtlos
in die reale Welt einzubetten. Durch das Tragen der AR-Brille hat der Benutzer
die Freiheit, sich in der Umgebung zu bewegen und die AR-Erfahrung ohne
Einschränkungen zu erleben.

\subsection{Benutzerschnittstellen und Interaktion}
Die Benutzerschnittstellen und Interaktion spielen eine zentrale Rolle im
Bereich der Augmented Reality (AR) und tragen maßgeblich zur intuitiven
Bedienbarkeit, Benutzerfreundlichkeit und Immersion von AR-Anwendungen bei.

Eine der gängigsten Formen der Benutzerschnittstelle in AR-Anwendungen ist das
Head-Mounted Display (HMD), das dem Benutzer ermöglicht, die virtuellen Inhalte
direkt vor seinen Augen zu sehen. Das HMD kann mit Sensoren ausgestattet sein,
um die Umgebung, sowie Bewegungen des Nutzers zu erkennen. Dies ermöglicht eine
Reihe von Interaktionsmöglichkeiten mit der AR-Anwendung.
\begin{itemize}
      \item 2D User Interfaces: Bei dieser Form der Interaktion,
            werden physische Knöpfe und Tasten verwendet, um mit den virtuellen Inhalten zu interagieren.
            Dies umfasst beispielsweise das Auswählen von Objekten, das Ausführen von Aktionen oder das Navigieren in Menüs. Auch Touch-Eingaben sind möglich.

      \item Für eine erweiterte Interaktionsmöglichkeit werden 3D-Benutzerschnittstellen
            verwendet, die Manipulationen an Objekten mit sechs Freiheitsgraden
            ermöglichen. Dies umfasst das Bewegen, Drehen und Skalieren von Objekten. Die
            Eingabegeräte selbst können unterschiedliche Formen annehmen, wie
            beispielsweise 3D-Mäuse oder Stäbe.

      \item Gestensteuerung: Bei dieser fortschrittlichen Form der Interaktion werden
            Gesten, Handbewegungen und auch Kopfbewegungen verwendet, um mit den virtuellen
            Inhalten zu interagieren. Dies umfasst das Zeigen auf Objekte, das Ziehen und
            Drehen von Objekten sowie das Ausführen von Gesten wie Pinch-to-Zoom. Darüber
            hinaus können Kopfbewegungen zur Steuerung von Menüs, zum Navigieren durch
            virtuelle Umgebungen oder zum Anpassen von Blickwinkeln genutzt werden. Durch
            die Einbindung von Kopfbewegungen in die Gestensteuerung wird eine natürlichere
            und immersivere Interaktion mit den AR-Inhalten ermöglicht. So kann der
            Benutzer beispielsweise durch Drehen des Kopfes seine Perspektive in der
            erweiterten Realität ändern oder durch Kopfbewegungen Menüoptionen auswählen.
            Diese erweiterte Form der Gestensteuerung trägt dazu bei, die Interaktion mit
            AR-Inhalten intuitiver und realitätsnäher zu gestalten. Auch Eye-Tracking ist
            hierbei möglich, um Objekte durch das Ansehen auszuwählen.

      \item Sprachbefehle: Auch Sprachbefehle können genutz werden, um mit Objekten im Raum
            zu interagieren. Benutzer können bestimmte Sprachbefehle verwenden, um Aktionen
            auszuführen, Objekte zu steuern oder Informationen abzurufen. Diese Art der
            Interaktion kann besonders nützlich sein, wenn die Hände des Benutzers
            beschäftigt sind.
\end{itemize}

Die Benutzerschnittstellen und Interaktionsmethoden in der AR werden
kontinuierlich weiterentwickelt, um die Benutzererfahrung zu verbessern und
neue Möglichkeiten der Interaktion zu erschließen. Durch die Integration von
fortgeschrittenen Technologien wie maschinellem Lernen und Künstlicher
Intelligenz ist es möglich, natürlichere und kontextbezogene Interaktionen zu
ermöglichen. Die Gestaltung der Benutzerschnittstellen und Interaktionsmethoden
spielt eine entscheidende Rolle, um AR-Anwendungen zugänglich,
benutzerfreundlich und ansprechend zu gestalten und den Benutzern ein
immersives und interaktives Erlebnis zu bieten.